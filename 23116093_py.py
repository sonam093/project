# -*- coding: utf-8 -*-
"""23116093.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_eDJJse4ELBvC_3bcgLD6lV4bvGt_zxC
"""

!pip install streamlit pyngrok

!pip install pdfplumber python-docx pytesseract Pillow transformers torch spacy streamlit pyngrok
!python -m spacy download en_core_web_sm

import streamlit as st
import pdfplumber
from docx import Document
from PIL import Image
import pytesseract
from transformers import pipeline
import spacy
from datetime import datetime

# Extraction functions
def extract_text_from_pdf(file):
    with pdfplumber.open(file) as pdf:
        return "".join([page.extract_text() or "" for page in pdf.pages])

def extract_text_from_docx(file):
    doc = Document(file)
    return "\n".join([para.text for para in doc.paragraphs])

def extract_text_from_txt(file):
    return file.read().decode('utf-8')

def extract_keywords(text, top_n=5):
    doc = nlp(text)
    freq = {}
    for chunk in doc.noun_chunks:
        word = chunk.text.strip()
        freq[word] = freq.get(word, 0) + 1
    sorted_keywords = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [kw[0] for kw in sorted_keywords[:top_n]]

def extract_text_from_image(file):
    image = Image.open(file)
    return pytesseract.image_to_string(image)


# Metadata functions
def extract_keywords(text, top_n=5):
    doc = nlp(text)
    freq = {}
    for chunk in doc.noun_chunks:
        word = chunk.text.strip()
        freq[word] = freq.get(word, 0) + 1
    sorted_keywords = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [kw[0] for kw in sorted_keywords[:top_n]]


def generate_summary(text):
    summary = summarizer(text, max_length=130, min_length=30, do_sample=False)
    return summary[0]['summary_text']

def generate_metadata(text, file_name, author=None):
    return {
        "title": file_name,
        "author": author or "Unknown",
        "date_created": datetime.now().strftime("%Y-%m-%d"),
        "keywords": extract_keywords(text),
        "summary": generate_summary(text),
        "document_type": "Unknown"
    }

# Commented out IPython magic to ensure Python compatibility.
# # Load NLP Models
# %%writefile nlp.py
# def generate_metadata(text):
#     return {
#         "Title": text[:50] + "..." if len(text) > 50 else text,
#         "Word Count": len(text.split()),
#         "Character Count": len(text),
#     }

# Streamlit App UI
st.set_page_config(page_title="Metadata Generator", layout="centered")
st.title("üìÑ Automated Metadata Generator")
st.write("Upload a PDF, DOCX, TXT, or Image file to extract metadata.")

uploaded_file = st.file_uploader("Upload a document", type=["pdf", "docx", "txt", "png", "jpg", "jpeg"])

if uploaded_file:
    file_name = uploaded_file.name
    ext = file_name.split('.')[-1].lower()

    st.info(f"Processing `{file_name}` ...")

    try:
        if ext == 'pdf':
            text = extract_text_from_pdf(uploaded_file)
        elif ext == 'docx':
            text = extract_text_from_docx(uploaded_file)
        elif ext == 'txt':
            text = extract_text_from_txt(uploaded_file)
        elif ext in ['png', 'jpg', 'jpeg']:
            text = extract_text_from_image(uploaded_file)
        else:
            st.error("Unsupported file type.")
            st.stop()

        st.success("‚úÖ Text extracted successfully!")
        st.subheader("üìÉ Extracted Text (first 1000 characters)")
        st.text_area("Text Preview", text[:1000], height=250)

        if st.button("Generate Metadata"):
            with st.spinner("Generating metadata..."):
                metadata = generate_metadata(text, file_name)
                st.subheader("üìë Generated Metadata")
                st.json(metadata)

    except Exception as e:
        st.error(f"‚ùå Error processing file: {str(e)}")

!pip install streamlit

!pip install pyngrok

!pip install --user streamlit

# Step 1: Install everything needed
!pip install --quiet --user streamlit pyngrok pdfplumber python-docx pytesseract Pillow transformers torch
!python -m spacy download en_core_web_sm

# Save your script (replace with your actual Streamlit code if needed)
# Save your script with OCR support added
with open("app.py", "w") as f:
    f.write("""
import streamlit as st
import pdfplumber
from docx import Document
from PIL import Image
import pytesseract
from transformers import pipeline
import spacy
from datetime import datetime

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
nlp = spacy.load("en_core_web_sm")

def extract_text_from_pdf(file):
    with pdfplumber.open(file) as pdf:
        return "".join([page.extract_text() or "" for page in pdf.pages])

def extract_text_from_docx(file):
    doc = Document(file)
    return "\\n".join([para.text for para in doc.paragraphs])

def extract_text_from_txt(file):
    return file.read().decode('utf-8')

def extract_text_from_image(file):
    image = Image.open(file)
    return pytesseract.image_to_string(image)

def extract_keywords(text, top_n=5):
    doc = nlp(text)
    freq = {}
    for chunk in doc.noun_chunks:
        word = chunk.text.strip()
        freq[word] = freq.get(word, 0) + 1
    sorted_keywords = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    return [kw[0] for kw in sorted_keywords[:top_n]]

def generate_summary(text):
    summary = summarizer(text, max_length=130, min_length=30, do_sample=False)
    return summary[0]['summary_text']

def generate_metadata(text, file_name, author=None):
    return {
        "title": file_name,
        "author": author or "Unknown",
        "date_created": datetime.now().strftime("%Y-%m-%d"),
        "keywords": extract_keywords(text),
        "summary": generate_summary(text),
        "document_type": "Unknown"
    }

st.set_page_config(page_title="Metadata Generator", layout="centered")

st.title("üìÑ Automated Metadata Generator")
st.write("Upload a PDF, DOCX, TXT, or Image file to extract metadata.")

uploaded_file = st.file_uploader("Upload Document", type=['pdf', 'docx', 'txt', 'jpg', 'jpeg', 'png'])

if uploaded_file:
    file_name = uploaded_file.name
    ext = file_name.split('.')[-1].lower()

    st.info(f"Processing `{file_name}` ...")

    try:
        if ext == 'pdf':
            text = extract_text_from_pdf(uploaded_file)
        elif ext == 'docx':
            text = extract_text_from_docx(uploaded_file)
        elif ext == 'txt':
            text = extract_text_from_txt(uploaded_file)
        elif ext in ['jpg', 'jpeg', 'png']:
            text = extract_text_from_image(uploaded_file)
        else:
            st.error("Unsupported file type.")
            st.stop()

        st.success("‚úÖ Text extracted successfully!")
        st.subheader("üìÉ Extracted Text (first 1000 characters)")
        st.text_area("Text Preview", text[:1000], height=250)

        if st.button("Generate Metadata"):
            with st.spinner("Generating metadata..."):
                metadata = generate_metadata(text, file_name)
                st.subheader("üìë Generated Metadata")
                st.json(metadata)

    except Exception as e:
        st.error(f"‚ùå Error processing file: {str(e)}")
""")

from pyngrok import ngrok, conf

# Paste your actual authtoken in quotes
authtoken = "2yoxRVcDe1U2wYISjH89jXhOK9P_QGBXbkhvvY8nR6zoXS4d"

conf.get_default().auth_token = authtoken
ngrok.set_auth_token(authtoken)

import os
from pyngrok import conf, ngrok

# Set ngrok auth token (only once per session)
conf.get_default().auth_token = "2yoxRVcDe1U2wYISjH89jXhOK9P_QGBXbkhvvY8nR6zoXS4d"
ngrok.set_auth_token("2yoxRVcDe1U2wYISjH89jXhOK9P_QGBXbkhvvY8nR6zoXS4d")

# Kill existing tunnels before starting a new one
ngrok.kill()

# Run Streamlit from user path in the background
os.system("~/.local/bin/streamlit run app.py &")

# Create a new tunnel
public_url = ngrok.connect(8501)
print("‚úÖ Streamlit app is live at:", public_url)

# Commented out IPython magic to ensure Python compatibility.
!mkdir -p metadata-generator-app
# %cd metadata-generator-app

!mkdir metadata-generator-app

!mv app.py metadata-generator-app/

with open("metadata-generator-app/requirements.txt", "w") as f:
    f.write("""
streamlit
pdfplumber
python-docx
pytesseract
Pillow
transformers
torch
spacy
en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl
""")

with open("metadata-generator-app/README.md", "w") as f:
    f.write("""\
# üìÑ Metadata Generator App

This is a Streamlit web app that lets users upload `.pdf`, `.docx`, `.txt`, or image files and automatically generates metadata such as:
- üìå Summary (using transformers)
- üîë Keywords (using spaCy)
- üïì Creation Date
- üìÅ Document type

---

## üöÄ How to Run Locally

### 1. Clone the repository:

```bash
git clone https://github.com/yourusername/metadata-generator-app.git
cd metadata-generator-app

## Features

- Extract text from PDF, Word, TXT, or image
- Generate keywords using spaCy
- Summarize using transformers
- View metadata in a user-friendly UI
""")

# push to github
cd metadata-generator-app
git init
git remote add origin https://github.com/YOUR_USERNAME/YOUR_REPO_NAME.git
git add .
git commit -m "Initial commit"
git push -u origin main

# zip the folder
!zip -r metadata-generator-app.zip metadata-generator-app